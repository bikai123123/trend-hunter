import json
import requests
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import quote

# --- ğŸ“¡ v3.1 ç¨³å®šæƒ…æŠ¥æº ---
SOURCES = [
    {
        "category": "ç§‘æŠ€",
        "name": "ITä¹‹å®¶",
        "url": "https://www.ithome.com/rss/",
        "emoji": "âš¡",
        "max_items": 20
    },
    {
        "category": "è´¢ç»",
        "name": "æ¾æ¹ƒè´¢ç»",  # ã€æ›¿æ¢ã€‘æ–°æµª -> æ¾æ¹ƒ (UTF-8, æ›´ç¨³å®š)
        "url": "https://www.thepaper.cn/rss.jsp?sectionid=25951",
        "emoji": "ğŸ“ˆ",
        "max_items": 20
    },
    {
        "category": "æ—¶äº‹",
        "name": "ä¸­æ–°ç½‘",
        "url": "http://www.chinanews.com.cn/rss/importnews.xml",
        "emoji": "ğŸ›ï¸",
        "max_items": 20
    }
]

# --- ğŸ§  AI åˆ†æå†…æ ¸ ---
POLLINATIONS_URL = "https://text.pollinations.ai/{}"

def get_ai_summary(title, category):
    """AI æé€Ÿæ€»ç»“"""
    try:
        if category == "è´¢ç»":
            prompt = f"As a financial analyst, summarize market impact in 1 sentence (Chinese). Title: '{title}'"
        elif category == "æ—¶äº‹":
            prompt = f"Summarize event objectively in 1 sentence (Chinese). Title: '{title}'"
        else:
            prompt = f"Explain tech innovation in 1 sentence (Chinese). Title: '{title}'"
        
        target_url = POLLINATIONS_URL.format(quote(prompt))
        # è¶…æ—¶è®¾ä¸º 10 ç§’ï¼Œé˜²æ­¢é˜»å¡
        response = requests.get(target_url + "?model=openai", timeout=10)
        
        if response.status_code == 200:
            return response.text.strip()
        else:
            return "ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…"
    except:
        return "ç‚¹å‡»é˜…è¯»åŸæ–‡"

def fetch_rss_data(source_config):
    """RSS æŠ“å–å¼•æ“ (å¢å¼ºç‰ˆ)"""
    category = source_config['category']
    print(f"ğŸ“¡ è¿æ¥ [{category}] {source_config['name']}...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        resp = requests.get(source_config['url'], headers=headers, timeout=15)
        
        # ã€å…³é”®ä¿®å¤ã€‘ä¸è¦å¼ºåˆ¶ encoding='utf-8'ï¼Œ
        # è€Œæ˜¯ç›´æ¥æŠŠäºŒè¿›åˆ¶ content å–‚ç»™ ETï¼Œè®©å®ƒæ ¹æ® XML å¤´è‡ªåŠ¨è¯†åˆ«ç¼–ç  (GBK/UTF-8 é€šåƒ)
        
        if resp.status_code != 200:
            print(f"âŒ è¿æ¥å¤±è´¥: {resp.status_code}")
            return []

        # ä½¿ç”¨ resp.content (Bytes) è€Œä¸æ˜¯ resp.text (String)
        root = ET.fromstring(resp.content)
        
        # æ¾æ¹ƒæ–°é—»çš„ç»“æ„å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œåšé€šç”¨é€‚é…
        channel = root.find('channel')
        items = channel.findall('item')[:source_config['max_items']]

        results = []
        for i, item in enumerate(items):
            title = item.find('title').text
            link = item.find('link').text
            
            # è¿›åº¦æ‰“å°
            if i % 5 == 0: print(f"   -> å¤„ç†ç¬¬ {i+1} æ¡: {title[:10]}...")
            
            ai_text = get_ai_summary(title, category)
            ai_text = ai_text.replace("'", "").replace('"', '').replace("\n", "")
            if len(ai_text) > 50: ai_text = ai_text[:49] + "..."

            results.append({
                "title": title,
                "link": link,
                "category": category,
                "emoji": source_config['emoji'],
                "aiReason": ai_text
            })
            
            # è¿™é‡Œçš„ sleep ç¨å¾®è°ƒå°ä¸€ç‚¹ï¼Œä¿è¯ 60 æ¡èƒ½è·‘å®Œ
            time.sleep(0.8)
            
        return results

    except Exception as e:
        print(f"âŒ è§£æé”™è¯¯ ({category}): {e}")
        return []

def update_html(news_list):
    if not news_list: return
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            content = f.read()
        
        js_data = ""
        for i, item in enumerate(news_list):
            js_data += "                {\n"
            js_data += f"                    id: {i}, platform: '{item['category']}', title: '{item['title']}', link: '{item['link']}', price: 'News', sales: 'åˆšåˆš', score: {100-i}, emoji: '{item['emoji']}',\n"
            js_data += f"                    aiReason: '{item['aiReason']}'\n"
            js_data += "                },\n"

        pattern = r"(// DATA_START\n)(.*?)(// DATA_END)"
        if re.search(pattern, content, re.DOTALL):
            new_content = re.sub(pattern, f"\\1{js_data}                \\3", content, flags=re.DOTALL)
            with open("index.html", "w", encoding="utf-8") as f:
                f.write(new_content)
            print(f"ğŸ‰ æˆåŠŸæ›´æ–° {len(news_list)} æ¡æƒ…æŠ¥ï¼")
    except Exception as e:
        print(f"âŒ HTML å†™å…¥å¤±è´¥: {e}")

if __name__ == "__main__":
    all_news = []
    for source in SOURCES:
        data = fetch_rss_data(source)
        all_news.extend(data)
        time.sleep(2)
    update_html(all_news)
