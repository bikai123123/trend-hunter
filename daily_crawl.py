import json
import requests
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import quote

# --- ğŸ“¡ v3.0 é«˜å®¹é‡æƒ…æŠ¥æº ---
SOURCES = [
    {
        "category": "ç§‘æŠ€",
        "name": "ITä¹‹å®¶",
        "url": "https://www.ithome.com/rss/",
        "emoji": "âš¡",
        "max_items": 20  # æ‰©å®¹è‡³ 20 æ¡
    },
    {
        "category": "è´¢ç»",
        "name": "æ–°æµªè´¢ç»",
        "url": "http://rss.sina.com.cn/roll/finance/hot_roll.xml",
        "emoji": "ğŸ“ˆ",
        "max_items": 20
    },
    {
        "category": "æ—¶äº‹",
        "name": "ä¸­æ–°ç½‘",
        "url": "http://www.chinanews.com.cn/rss/importnews.xml",
        "emoji": "ğŸ›ï¸",
        "max_items": 20
    }
]

# --- ğŸ§  AI åˆ†æå†…æ ¸ ---
POLLINATIONS_URL = "https://text.pollinations.ai/{}"

def get_ai_summary(title, category):
    """AI æé€Ÿæ€»ç»“æ¨¡å¼"""
    try:
        if category == "è´¢ç»":
            prompt = f"As a financial analyst, summarize market impact in 1 sentence (Chinese). Title: '{title}'"
        elif category == "æ—¶äº‹":
            prompt = f"Summarize event objectively in 1 sentence (Chinese). Title: '{title}'"
        else:
            prompt = f"Explain tech innovation in 1 sentence (Chinese). Title: '{title}'"
        
        target_url = POLLINATIONS_URL.format(quote(prompt))
        # ç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œä¿è¯å¤§é‡æŠ“å–æ—¶çš„æ•´ä½“é€Ÿåº¦
        response = requests.get(target_url + "?model=openai", timeout=10)
        
        if response.status_code == 200:
            return response.text.strip()
        else:
            return "ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…"
    except:
        return "ç‚¹å‡»é˜…è¯»åŸæ–‡"

def fetch_rss_data(source_config):
    """RSS æŠ“å–å¼•æ“"""
    category = source_config['category']
    print(f"ğŸ“¡ è¿æ¥ [{category}] {source_config['name']} (ç›®æ ‡: 20æ¡)...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        resp = requests.get(source_config['url'], headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        
        if resp.status_code != 200:
            print(f"âŒ è¿æ¥å¤±è´¥: {resp.status_code}")
            return []

        root = ET.fromstring(resp.text)
        channel = root.find('channel')
        items = channel.findall('item')[:source_config['max_items']]

        results = []
        for i, item in enumerate(items):
            title = item.find('title').text
            # å…³é”®ï¼šæŠ“å–åŸæ–‡é“¾æ¥
            link = item.find('link').text
            
            # è¿›åº¦æ¡æ‰“å°ï¼Œé¿å…æ—¥å¿—å¤ªé•¿
            if i % 5 == 0: print(f"   -> æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡...")
            
            ai_text = get_ai_summary(title, category)
            ai_text = ai_text.replace("'", "").replace('"', '').replace("\n", "")
            if len(ai_text) > 50: ai_text = ai_text[:49] + "..."

            results.append({
                "title": title,
                "link": link,  # æ–°å¢å­—æ®µ
                "category": category,
                "emoji": source_config['emoji'],
                "aiReason": ai_text
            })
            
            # ç¨å¾®åŠ å¿«é€Ÿåº¦ï¼š1ç§’é—´éš” (60æ¡çº¦è€—æ—¶1åˆ†é’Ÿ)
            time.sleep(1)
            
        return results

    except Exception as e:
        print(f"âŒ è§£æé”™è¯¯: {e}")
        return []

def update_html(news_list):
    if not news_list: return
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            content = f.read()
        
        js_data = ""
        for i, item in enumerate(news_list):
            js_data += "                {\n"
            # å†™å…¥ link å­—æ®µ
            js_data += f"                    id: {i}, platform: '{item['category']}', title: '{item['title']}', link: '{item['link']}', price: 'News', sales: 'åˆšåˆš', score: {100-i}, emoji: '{item['emoji']}',\n"
            js_data += f"                    aiReason: '{item['aiReason']}'\n"
            js_data += "                },\n"

        pattern = r"(// DATA_START\n)(.*?)(// DATA_END)"
        if re.search(pattern, content, re.DOTALL):
            new_content = re.sub(pattern, f"\\1{js_data}                \\3", content, flags=re.DOTALL)
            with open("index.html", "w", encoding="utf-8") as f:
                f.write(new_content)
            print(f"ğŸ‰ æˆåŠŸæ›´æ–° {len(news_list)} æ¡æƒ…æŠ¥ï¼")
    except Exception as e:
        print(f"âŒ HTML å†™å…¥å¤±è´¥: {e}")

if __name__ == "__main__":
    all_news = []
    for source in SOURCES:
        all_news.extend(fetch_rss_data(source))
        time.sleep(2)
    update_html(all_news)
